================01b================
# 手动建立向量 c()
# 常用方法 length(), mode(), 取位[]
x=c(1,2,5,3,6,7,3)	#建立向量
x						#查看向量
'[1] 1 2 5 3 6 7 3'
mode(x)				#查看数据类型
'[1] "numeric"'
length(x)				#查看向量长度
'[1] 7'
x[3]					#查看指定位数据
'[1] 5'

y=c('a','u','e')
mode(y)
'[1] "character"'

z=c(1:100)				#简写法1,2,...100

# rbind(), cbind() 行,列合并
y=c(5,3,6,8,9,8,3)

rbind(x,y)							#x, y按行合并
' [,1] [,2] [,3] [,4] [,5] [,6] [,7]
x    1    2    5    3    6    7    3
y    5    3    6    8    9    8    3
'
cbind(x,y)							#x, y按列合并
'    x y
[1,] 1 5
[2,] 2 3
[3,] 5 6
[4,] 3 8
[5,] 6 9
[6,] 7 8
[7,] 3 3
'
# 常用统计函数
mean(x)
'[1] 3.857143'
sum(x)
'[1] 27'
max(x)
'[1] 7'
min(x)
'[1] 1'
var(x)
'[1] 4.809524'
sd(x)
'[1] 2.193063'
prod(x)		#连乘
'[1] 3780'

# 帮助
help(matrix)

# 批量产生向量
1:5
'[1] 1 2 3 4 5'
1:5-1
'[1] 0 1 2 3 4'
1:5*2
'[1]  2  4  6  8 10'

a<-1:10*2
a
 '[1]  2  4  6  8 10 12 14 16 18 20'
a[4]							#只选择第4个元素
'[1] 8'
a[-4]							#不显示第4个元素
'[1]  2  4  6 10 12 14 16 18 20'
a[2:5]						#只显示2-5个元素
'[1]  4  6  8 10'
a[-(2:5)]						#不显示第2-5个元素
'[1]  2 12 14 16 18 20'
a[c(2,4,7)]					#显示第2, 4, 7个元素, 如果直接a[2, 4, 7]表示2行,4列,7层的那个元素, 此例中产生找不到对象错误
'[1]  4  8 14'
a[a<12]						#显示符合条件的元素
'[1]  2  4  6  8 10'
a[a<12&a>5]
'[1]  6  8 10'

#  批量产生向量seq()
seq(1, 10)
 '[1]  1  2  3  4  5  6  7  8  9 10'
seq(5, 54, by=5)				#指定步长
 '[1]  5 10 15 20 25 30 35 40 45 50'
seq(5, 54, length=4)			#指定等分点数
'[1]  5.00000 21.33333 37.66667 54.00000'

#  批量产生字符元素向量letters()
letters[5:28]
 '[1] "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s" "t" "u" "v" "w" "x" "y" "z" NA  NA '

# 寻找符合条件的元素下标(不是求元素内容) which()
a
' [1]  2  4  6  8 10 12 14 16 18 20'
which.max(a)					#符合条件(最大值)的元素在第10号位. 从1开始计数
'[1] 10'
max(a)						#最大值为20
'[1] 20'
which(a==12)
'[1] 6'
which(a>13)
'[1]  7  8  9 10'

# 排序 sort(), 倒序 rev()
a <- c(2,5,7,8,3,2,4,6)
sort(a)					#并不真正改变a的值, 如需要取得排序后的结果, 可以把排序结果赋值给新变量
'[1] 2 2 3 4 5 6 7 8'
rev(a)			
'[1] 6 4 2 3 8 7 5 2'

# 产生矩阵 matrix()
a <-c(1:12)
matrix(a, nrow=3, ncol=4)		#默认按列进行填充
'     [,1] [,2] [,3] [,4]
[1,]    1    4    7   10
[2,]    2    5    8   11
[3,]    3    6    9   12
'
matrix(a, nrow=3, ncol=4, byrow=T)		# 也可matrix(a, nrow=3, ncol=4, byrow=TRUE)
 '    [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    5    6    7    8
[3,]    9   10   11   12
'
matrix(a, nrow=3, ncol=3)		#多出部分元素被抛弃
 '    [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
[3,]    3    6    9
'
# 矩阵转置 t()
b<-matrix(a, nrow=3, ncol=4, byrow=T)
b
'     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    5    6    7    8
[3,]    9   10   11   12
'
t(b)
'     [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12
'
# 矩阵点运算
b
 '    [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    5    6    7    8
[3,]    9   10   11   12
'
b+b
 '    [,1] [,2] [,3] [,4]
[1,]    2    4    6    8
[2,]   10   12   14   16
[3,]   18   20   22   24
'
b-b
 '    [,1] [,2] [,3] [,4]
[1,]    0    0    0    0
[2,]    0    0    0    0
[3,]    0    0    0    0
'
b*2
 '    [,1] [,2] [,3] [,4]
[1,]    2    4    6    8
[2,]   10   12   14   16
[3,]   18   20   22   24
'
b-1
 '    [,1] [,2] [,3] [,4]
[1,]    0    1    2    3
[2,]    4    5    6    7
[3,]    8    9   10   11
'
# 矩阵运算
a=matrix(1:12, nrow=3, ncol=4)
b=matrix(1:12, nrow=4, ncol=3)
a%*%b
 '    [,1] [,2] [,3]
[1,]   70  158  246
[2,]   80  184  288
[3,]   90  210  330
'
# 找对角线值 diag()
b
 '    [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12
'
diag(b)					#如果输入diag的参数为矩阵, 则重载为求出对角线元素
'[1]  1  6 11'

diag(diag(b))			#如果输入diag的参数为向量, 则重载为建立对角矩阵
 '    [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    6    0
[3,]    0    0   11
'
diag(4)					# 如果输入diag的参数单元素数字, 则重载为建立该数字为阶的单位矩阵
 '    [,1] [,2] [,3] [,4]
[1,]    1    0    0    0
[2,]    0    1    0    0
[3,]    0    0    1    0
[4,]    0    0    0    1
'
# 求逆矩阵 solve()
rnorm(16)			# 产生16个正态分布随机数字

a=matrix(rnorm(16),4,4)
a
 '           [,1]       [,2]      [,3]       [,4]
[1,] -0.58160109  0.6909260 -1.540300  0.5687638
[2,]  0.04836995 -0.7788544  1.256786 -0.3495563
[3,] -0.02846253 -0.1610256  0.325031  0.5062237
[4,]  0.17209137 -0.1804786  1.177659 -2.2172371
'
solve(a)
  '           [,1]       [,2]      [,3]       [,4]
[1,] -1.945932937 -1.1251943 -2.027675 -0.7847219
[2,] -0.008629116 -2.5978638  4.667320  1.4729591
[3,]  0.032540125 -0.8640855  3.310009  0.9002914
[4,] -0.133048335 -0.3348202  1.220784 -0.1536348
'
b						# a11*x1+a12*x2+a13*x3+a14*x4=b1...的方程组
'[1] 1 2 3 4'
solve(a,b)
'[1] -13.418233  14.689439  11.835563   2.245123'

# 求矩阵特征值 eigen()
a=diag(4)+1
a
'     [,1] [,2] [,3] [,4]
[1,]    2    1    1    1
[2,]    1    2    1    1
[3,]    1    1    2    1
[4,]    1    1    1    2
'
a.e=eigen(a, symmetric=T)
a.e
'$values
[1] 5 1 1 1
$vectors
     [,1]       [,2]       [,3]       [,4]
[1,] -0.5  0.8660254  0.0000000  0.0000000
[2,] -0.5 -0.2886751 -0.5773503 -0.5773503
[3,] -0.5 -0.2886751 -0.2113249  0.7886751
[4,] -0.5 -0.2886751  0.7886751 -0.2113249
'
================01c================

# 向量和数组的关系
数组就是加了维度的向量, 比如一维数组就是向量; 二维数组就是矩阵; 三维数组就是array(,,)
x<-c(1:6)
is.vector(x)
'[1] TRUE'
is.array(x)
'[1] FALSE'
is.matrix(x)
'[1] FALSE'

# 将x向量转化为数组
dim(x)<-c(2,3)
is.vector(x)
'[1] FALSE'
is.array(x)
'[1] TRUE'
is.matrix(x)		#矩阵是数组的一个特例
'[1] TRUE'

# 数据框和矩阵关系 data.frame()
矩阵中每个元素都必然是数字, 数据框可以是各种类型
x <- c(1:6)
y <- x*2
data.frame(x,y)		# 按列合并
 ' x  y
1 1  2
2 2  4
3 3  6
4 4  8
5 5 10
6 6 12
'
# 添加表头
z=data.frame('var1'=x, 'var2'=y)
z
'  var1 var2
1    1    2
2    2    4
3    3    6
4    4    8
5    5   10
6    6   12
'
# 绘制矩阵的点图 plot()
plot(z)

# 读文本文件 read.table()
x=read.table("E:/Google Drive/!2015Spr/data.txt")
x=read.table("E:/Google Drive/!2015Spr/data.prn")	#prn是excel另存为以空格为分隔符的文本文件
# 读csv文件
x=read.csv("E:/Google Drive/!2015Spr/data.csv")	#csv是excel另存为以','为分隔符的文本文件

# 读剪贴板中内容
x=read.table("clipboard",header=FALSE)		#clipboard是固定指向剪贴板标识, 不可更改

# 读Excel文件 RODBC.odbcConnectExcel()
step1,
install.packages("RODBC")

step2,
library(RODBC)								# 该方法不太灵光: odbcConnectExcel is only usable with 32-bit Windows
z <- odbcConnectExcel("E:\\Google Drive\\!2015Spr\\data.xlsx")
'Error in odbcConnectExcel("E:\\Google Drive\\!2015Spr\\data.xlsx") : 
  odbcConnectExcel is only usable with 32-bit Windows
'
w<-sqlFetch(z, "sheet1")

# 循环语句 for
for (i in 1:15) {
	a[i]=i*2+3
}
a
' [1]  5  7  9 11 13 15 17 19 21 23 25 27 29 31 33'

# 循环语句 while
a[1]=5
i=1
while (a[i]<50){
	i=i+1;
	a[i]=a[i-1]+2
}
a
' [1]  5  7  9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 41 43 45 47 49 51'
 
# 使用R脚本 source()
source("D:\\mysource.r")

# mysource.r的内容
a[1]=5
i=1
while (a[i]<50){
	i=i+1
	a[i]=a[i-1]+2
}
print(a)		#注意区别, 脚本中需要用print()方法显示, 而不能直接输入变量名
 
# 产生随机数
均匀分布 runif()
正态分布	rnorm()
泊松分布	rpois()
指数分布 rexp()
Gamma分布 rgamma()
二项式分布 rbinom()
几何分布 rgeom()

x1=round(runif(20, min=80, max=100))
x2=round(rnorm(20, mean=83, sd=18))
x2[which(x2>100)]=100						# 将大于100的数字trim成100
 
# 写R内存中的文件到硬盘上 write.table()
x=data.frame(x1,x2)
write.table(x, file="d:\\mydata.csv", col.name=FALSE, row.name=FALSE, sep=',', append=TRUE)

# 求平均值 colMeans()
colMeans(x)
'   x1    x2 
89.09 79.96 '
colMeans(x)[c("x1")]
 '  x1 
89.09 '

# apply()
apply(x, 2, mean)				#对x矩阵, 1代表行操作, 2代表列操作, 效率非常高, 可以加function取代for循环
apply(x, 2, sd)
apply(x, 1, max)

apply(x[c("x1","x2")], 1, sum)		#对指定列的数据分行求和, 也就是求每个obs的和
which.max(apply(x[c("x1","x2")], 1, sum))		#找出求和最大的obs是谁
x$x1[which.max(apply(x[c("x1","x2")], 1, sum))]			#x$x1指x中的x1列, 这里是找出which结果条目的x1的值

================02a================

# 直方图 hist()
hist(x$x2)
# 散点图 plot()
plot(x$x1, x$x2)
# 柱状图 table(), barplot()
table (x$x1)
' 80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100 
  3   7   7   6   1   6   8   4   4   8   3   5   5   7   7   6   1   6   1   4    1'
barplot(table(x$x1))
# 饼图 pie()
pie(table(x$x1))
# 箱形图 boxplot()
boxplot(x$x1, x$x2)
boxplot(x[1:2], col=c("red","green"), notch=TRUE)
boxplot(x[1:2], col=c("red","green"), notch=TRUE)
boxplot(x[1:2], col=c("red","green"), notch=TRUE, horizontal=TRUE)

================02b================

# 绘图的控制
plot(x$x1,x$x2,
	main="chart title",
	xlab="x-axis lable",
	ylab="y-axis lable",
	xlim=c(0,100),		#坐标轴有100分隔
	ylim=c(0,100),
	xaxs="i",			#设置坐标轴风格
	col="red",			#设置点的颜色
	pch=19 				#设置点的symbol, 17=triangle, 18=dimond, 19=circle
)

# 折线图
a=c(2,3,4,5,6)
b=c(4,7,8,10,11)
plot(a,b, type='l', col="green", lwd=2)			#比之前plot多了一个type参数用以连接各点

# 多条曲线在一图上 lines()
a=c(2,3,4,5,6)
b=c(4,7,8,10,11)
plot(a,b, type='l', col="green", lwd=2)	
c=c(1,3,4,8,8,10)
lines(c, type='l', col="red", lwd=2)			#lines()可以在plot图上继续添加图像. 必须先有plot的图像为基础

# 查看R内置数据集 data()

# 热力图 heatmap()
heatmap(
	as.matrix(mtcars),			# 数据转换成matrix格式
	Rowv=NA,
	Colv=NA,
	col=heat.colors(256),
	scale='column',
	margins=c(2,8)
)

# 散点图集
pairs(iris[,1:4])			#遍历样本中全部配对的二元图, iris[,1:4]代表取iris表中1到4列作图
plot(iris[,1:4])

par(mfrow=c(3,1))							# 设置输出图形为3行1列的布局形式
plot(x1,x2); plot(x2,x3); plot(x3,x1)		# 画出的图形将按上述设置进行排版

# 查看颜色常数
colors()

# 多层图形绘制
x1=c(1,2,3)
x2=c(4,2,6)
x3=(3,4,2,5)
x4=c(5,4,9,7)

# 图形窗口的管理
dev.new()					# 建立一个新的窗口
plot(x1,x2)					# 此时的图形将在新建的窗口中绘制

dev.list()					# 显示当前有多少个图形窗
dev.cur()					# 显示当前图形窗, 所绘制的图形将显示在这个窗口中

dev.new()
dev.next()					# 只是告知下一个窗口是什么, 并不改变窗口焦点
dev.prev()

dev.off(which=dev.cur())	# 关闭当前焦点窗口
graphics.off()				# 全部关闭

# 3维绘图
install.packages("scatterplot3d")
library(scatterplot3d)
scatterplot3d(mtcars[1:3])

x=y=seq(-2*pi, 2*pi, pi/15)				# seq(from = 1, to = 1, by = ((to - from)/(length.out - 1)))
f=function(x,y) sin(x)*sin(y)			# 自定义函数
z=outer(x,y,f)
# The outer product of the arrays X and Y is the array A with dimension c(dim(X), dim(Y))
# where element A[c(arrayindex.x, arrayindex.y)] = FUN(X[arrayindex.x], Y[arrayindex.y], ...).
persp(x,y,z, theta=30, phi=30, expand=0.7, col="lightblue")

# 画地图
install.packages("maps")
library(maps)
map("state", interior=FALSE)						# 只有一个地图外轮廓
map("state", interior=TRUE, col="red", add=TRUE)	# 在之前基础上添加州边界
map("world", fill=TRUE, col=heat.colors(10))
map("state", interior=TRUE, col="red", add=TRUE)	# 此时美国地区的州被添加到地图上

# 绘图例子
# 设置图片边界, 只显示美国地区附近
xlim=c(-171.738281, -56.601563)
ylim=c(12.039321, 71.856229)
map("world", col="#f2f2f2", fill=TRUE, bg="white", lwd=0.05, xlim=xlim, ylim=ylim)

# 绘制一条两点的连接线
install.packages("geosphere")
library(geosphere)

lat_me = 45.21300
lon_me = -68.906520

lat_ca = 39.164141
lon_ca = -121.64062
inter1 = gcIntermediate(c(lon_ca, lat_ca),
					  c(lon_me, lat_me),
					  n=50,					# The requested number of points on the Great Circle
					  addStartEnd=TRUE)
lines(inter)

lat_tx = 29.954935
lon_tx = -98.701172
inter2 = gcIntermediate(c(lon_ca, lat_ca),
						c(lon_tx, lat_tx),
						n=50,
						addStartEnd=TRUE)
lines(inter2, col="red")

# 装载数据
airports = read.csv("http://datasets.flowingdata.com/tuts/maparcs/airports.csv", header=TRUE)
flights  = read.csv("http://datasets.flowingdata.com/tuts/maparcs/flights.csv", header=TRUE, as.is=TRUE)

# 用大量数据画图
fsub = flights[flights$airline=="AA",]							# 只取AA公司的信息, 所有行列
for (j in 1:length(fsub$airline)){
	air1 = airports[airports$iata == fsub[j,]$airport1,]		# 对sub中一个obs, 取出fsub中airport1短码(iata), 然后在airports表中查该条目完整信息
	air2 = airports[airports$iata == fsub[j,]$airport2,]
	
	inter = gcIntermediate(c(air1[1,]$long, air1[1,]$lat),		# 取出之前对应完整机场信息中的long, lat数据
						   c(air2[1,]$long, air2[1,]$lat),
						   n=100,
						   addStartEnd=TRUE)
	lines(inter, col="black", lwd=0.8) 
}
 
================03c================

# 产生随机数
正态分布		rnorm(n, mean=0, sd=1)
指数分布		rexp(n, rate=1)
Gamma分布	rgamma(n, shape, scale=1)
泊松分布		rpois(n, lambda)
Weibull分布	rweibull(n, shape, scale=1)
beta分布		rbetaa(n, shape1, shape2)
rt分布		rt(n, df)
F分布		rf(n, df1, df2)
K-square分布	rchisq(n, df)
二项式分布	rbinom()
几何分布		rgeom()
超几何分布	rhyper(nn, m, n, k)
logistic分布	rlogis(n, location=0, scale=1)
lognorm分布	rlnorm(n, meanlog=0, sdlog=1)
均匀分布		runif()

# 分位数 quantile()
quantile(x)
'       0%       25%       50%       75%      100% 
-6.283185 -3.141593  0.000000  3.141593  6.283185 
'
quantile(x, probs=seq(0,1,0.2))
'       0%       20%       40%       60%       80%      100% 
-6.283185 -3.769911 -1.256637  1.256637  3.769911  6.283185 
'
# 协方差cov(), 相关系数cor()		22:45
x1=rnorm(10, mean=0, sd=1)
x2=rexp(10, rate=1)
cov(x1, x2)
'[1] 0.1226566'
cor(x1, x2)
'[1] 0.4030629'

'
协方差就是用来描述二维随机变量X与Y相互关联程度的一个特征数. 协方差cov(X,Y)是有量纲的量
cov(X,Y)=E{ [ X-E(X) ]*[ Y-E(Y) ] }
cov(X,X)=var(X))

为了消除量纲的影响, 对协方差除以相同量纲的量, 就得到 相关系数cor
相关系数介于-1到1之间
'
================03d================

# 相关分析例子
i1 = iris[which(iris$Species=="setosa"),1:2]
plot(i1)
cor(i1[1], i1[2])				

             Sepal.Width
Sepal.Length   0.7425467

# 还需要对sample代表poplution性进行假设检验, 即求CL=95%范围值
cor.test(i1$Sepal.Length, i1$Sepal.Width)
'
        Pearson s product-moment correlation

data:  i1$Sepal.Length and i1$Sepal.Width
t = 7.6807, df = 48, p-value = 6.71e-10
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.5851391 0.8460314
sample estimates:
      cor 
0.7425467 
'

# 线性回归 lm()
h=c(171,175,159,155,152,158,154,164,168,166,159,164)
w=c(57,64,41,38,35,44,41,51,57,49,47,46)

lreg=lm(w ~ h)							# w ~ h是回归公式. w 和 h 的线性关系. a中存放了模型对象
lreg.noIntercept = lm(w ~ h+0)			# 此时表示忽略常数项, 也可写成 w ~ h-0, w~ h-1
formula(lreg)							# 查看a中的模型公式
' w ~ 1 + h '
summary(lreg)
'
Call:
lm(formula = w ~ h)

Residuals:
   Min     1Q Median     3Q    Max 
-3.721 -1.699  0.210  1.807  3.074 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -140.3644    17.5026   -8.02 1.15e-05 ***
h              1.1591     0.1079   10.74 8.21e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.546 on 10 degrees of freedom
Multiple R-squared:  0.9203,    Adjusted R-squared:  0.9123 
F-statistic: 115.4 on 1 and 10 DF,  p-value: 8.21e-07
'

# 方差分析 anova()
anova(lreg)
'
Analysis of Variance Table

Response: w
          Df Sum Sq Mean Sq F value   Pr(>F)    
h          1 748.17  748.17  115.41 8.21e-07 ***
Residuals 10  64.83    6.48                     
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
'

# 残差分析 residuals()
residuals(lreg)
'
         1          2          3          4          5          6          7          8          9         10         11         12 
-0.8349544  1.5288044 -2.9262307 -1.2899895 -0.8128086  1.2328296  2.8690708  1.2784678  2.6422265 -3.0396529  3.0737693 -3.7215322
'
# 预测 predict()
z=data.frame(x=185)
predict(lreg, z)
'
      1
74.0618
'
predict(lreg, z, interval="prediction", level=0.95)		# xxx有点问题xxx
'
        fit      lwr      upr
1   74.0618  65.9862 82.13739
'

================04d================

# multi-variate 选择关键variables例子1
swiss
s = lm(Fertility ~ . ,data = swiss)		# 直接使用所有attributes作为var(除了Fertility)
summary (s)
'
Call:
lm(formula = Fertility ~ ., data = swiss)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.2743  -5.2617   0.5032   4.1198  15.3213 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)      66.91518   10.70604   6.250 1.91e-07 ***
Agriculture      -0.17211    0.07030  -2.448  0.01873 *  
Examination      -0.25801    0.25388  -1.016  0.31546    
Education        -0.87094    0.18303  -4.758 2.43e-05 ***
Catholic          0.10412    0.03526   2.953  0.00519 ** 
Infant.Mortality  1.07705    0.38172   2.822  0.00734 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 7.165 on 41 degrees of freedom
Multiple R-squared:  0.7067,    Adjusted R-squared:  0.671 
F-statistic: 19.76 on 5 and 41 DF,  p-value: 5.594e-10
'

# multi-variate 选择关键variables例子2(仅为演示使用方法, 实际应使用ANOVA选择关键var再建立模型)
toothpaste<-data.frame(
	X1=c(-0.05, 0.25,0.60,0, 0.25,0.20, 0.15,0.05,-0.15, 0.15,0.20, 0.10,0.40,0.45,0.35,0.30, 0.50,0.50, 0.40,-0.05,-0.05,-0.10,0.20,0.10,0.50,0.60,-0.05,0, 0.05, 0.55),
	X2=c( 5.50,6.75,7.25,5.50,7.00,6.50,6.75,5.25,5.25,6.00,6.50,6.25,7.00,6.90,6.80,6.80,7.10,7.00,6.80,6.50,6.25,6.00,6.50,7.00,6.80,6.80,6.50,5.75,5.80,6.80),
	Y =c( 7.38,8.51,9.52,7.50,9.33,8.28,8.75,7.87,7.10,8.00,7.89,8.15,9.10,8.86,8.90,8.87,9.26,9.00,8.75,7.95,7.65,7.27,8.00,8.50,8.75,9.21,8.27,7.67,7.93,9.26)
)
lm.sol<-lm(Y ~ X1+X2, data=toothpaste)
summary(lm.sol)
'
Call:
lm(formula = Y ~ X1 + X2, data = toothpaste)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.49779 -0.12031 -0.00867  0.11084  0.58106 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   4.4075     0.7223   6.102 1.62e-06 ***
X1            1.5883     0.2994   5.304 1.35e-05 ***
X2            0.5635     0.1191   4.733 6.25e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.2383 on 27 degrees of freedom
Multiple R-squared:  0.886,     Adjusted R-squared:  0.8776 
F-statistic:   105 on 2 and 27 DF,  p-value: 1.845e-13
'
# 试着通过调整模型提高R^2结果
lm.new<-update(lm.sol, .~.+I(X2^2))		#.~.表示原模型, 在此基础上添加一项
summary(lm.new)
'
Call:
lm(formula = Y ~ X1 + X2 + I(X2^2), data = toothpaste)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.40330 -0.14509 -0.03035  0.15488  0.46602 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  17.3244     5.6415   3.071  0.00495 ** 
X1            1.3070     0.3036   4.305  0.00021 ***
X2           -3.6956     1.8503  -1.997  0.05635 .  
I(X2^2)       0.3486     0.1512   2.306  0.02934 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.2213 on 26 degrees of freedom
Multiple R-squared:  0.9054,    Adjusted R-squared:  0.8945 
F-statistic: 82.94 on 3 and 26 DF,  p-value: 1.944e-13
'
# 试着去除X2项提高R^2结果(因为X2项insignificant)
lm2.new<-update(lm.new, .~.-X2)
summary(lm2.new)
'
Call:
lm(formula = Y ~ X1 + I(X2^2), data = toothpaste)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.4859 -0.1141 -0.0046  0.1053  0.5592 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  6.07667    0.35531  17.102 5.17e-16 ***
X1           1.52498    0.29859   5.107 2.28e-05 ***
I(X2^2)      0.04720    0.00952   4.958 3.41e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.2332 on 27 degrees of freedom
Multiple R-squared:  0.8909,    Adjusted R-squared:  0.8828 
F-statistic: 110.2 on 2 and 27 DF,  p-value: 1.028e-13
'
# 试着在Y ~ X1+X2+X2^2 基础上添加交互项X1X2提高R^2结果
lm3.new<-update(lm.new, .~.+X1*X2)
summary(lm3.new)
'
Call:
lm(formula = Y ~ X1 + X2 + I(X2^2) + X1:X2, data = toothpaste)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.43725 -0.11754  0.00489  0.12263  0.38410 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  29.1133     7.4832   3.890 0.000656 ***
X1           11.1342     4.4459   2.504 0.019153 *  
X2           -7.6080     2.4691  -3.081 0.004963 ** 
I(X2^2)       0.6712     0.2027   3.312 0.002824 ** 
X1:X2        -1.4777     0.6672  -2.215 0.036105 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.2063 on 25 degrees of freedom
Multiple R-squared:  0.9209,    Adjusted R-squared:  0.9083 
F-statistic: 72.78 on 4 and 25 DF,  p-value: 2.107e-13
'

# stepwise自动选择关键变量 step()
s = lm(Fertility ~ . ,data = swiss)
s1 = step(s, direction="backward")
'
Start:  AIC=190.69
Fertility ~ Agriculture + Examination + Education + Catholic + 
    Infant.Mortality

                   Df Sum of Sq    RSS    AIC
- Examination       1     53.03 2158.1 189.86		#该项AIC值低于之前步骤, 下一步中去除
<none>                          2105.0 190.69
- Agriculture       1    307.72 2412.8 195.10
- Infant.Mortality  1    408.75 2513.8 197.03
- Catholic          1    447.71 2552.8 197.75
- Education         1   1162.56 3267.6 209.36

Step:  AIC=189.86
Fertility ~ Agriculture + Education + Catholic + Infant.Mortality

                   Df Sum of Sq    RSS    AIC
<none>                          2158.1 189.86
- Agriculture       1    264.18 2422.2 193.29
- Infant.Mortality  1    409.81 2567.9 196.03
- Catholic          1    956.57 3114.6 205.10
- Education         1   2249.97 4408.0 221.43
'

# stepwise选择关键variables例子
cement<-data.frame(
	X1=c( 7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10),
	X2=c(26, 29, 56, 31, 52, 55, 71, 31, 54, 47, 40, 66, 68),
	X3=c( 6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8),
	X4=c(60, 52, 20, 47, 33, 22, 6, 44, 22, 26, 34, 12, 12),
	Y =c(78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5,93.1,115.9, 83.8, 113.3, 109.4)
)
lm.sol<-lm(Y ~ X1+X2+X3+X4, data=cement)
summary(lm.sol)
'
Call:
lm(formula = Y ~ X1 + X2 + X3 + X4, data = cement)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.1750 -1.6709  0.2508  1.3783  3.9254 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  62.4054    70.0710   0.891   0.3991  
X1            1.5511     0.7448   2.083   0.0708 .
X2            0.5102     0.7238   0.705   0.5009  
X3            0.1019     0.7547   0.135   0.8959  
X4           -0.1441     0.7091  -0.203   0.8441  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.446 on 8 degrees of freedom
Multiple R-squared:  0.9824,    Adjusted R-squared:  0.9736 
F-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07
'
lm.step<-step(lm.sol)
'
Start:  AIC=26.94
Y ~ X1 + X2 + X3 + X4

       Df Sum of Sq    RSS    AIC
- X3    1    0.1091 47.973 24.974		#可以看出去掉X3后AIC值可以下降, 程序自动去除该变量
- X4    1    0.2470 48.111 25.011
- X2    1    2.9725 50.836 25.728
<none>              47.864 26.944
- X1    1   25.9509 73.815 30.576

Step:  AIC=24.97
Y ~ X1 + X2 + X4

       Df Sum of Sq    RSS    AIC
<none>               47.97 24.974
- X4    1      9.93  57.90 25.420
- X2    1     26.79  74.76 28.742
- X1    1    820.91 868.88 60.629
'
summary(lm.step)
'
Call:
lm(formula = Y ~ X1 + X2 + X4, data = cement)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.0919 -1.8016  0.2562  1.2818  3.8982 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  71.6483    14.1424   5.066 0.000675 ***
X1            1.4519     0.1170  12.410 5.78e-07 ***
X2            0.4161     0.1856   2.242 0.051687 .  
X4           -0.2365     0.1733  -1.365 0.205395    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.309 on 9 degrees of freedom
Multiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 
F-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08
'

# 手动逐步选择关键变量方法 add1(), drop1()
# 在之前step()优化基础上, 如果还需要去除一个var, 则:
drop1(lm.step)
'
Single term deletions

Model:
Y ~ X1 + X2 + X4
       Df Sum of Sq    RSS    AIC
<none>               47.97 24.974
X1      1    820.91 868.88 60.629
X2      1     26.79  74.76 28.742
X4      1      9.93  57.90 25.420		# 去掉X4造成的AIC增加最小, 该需求下去除X4为最优方案
'
lm.opt<-lm(Y ~ X1+X2, data=cement); summary(lm.opt)		# 同一行内多个命令, JRI调用中有用
'
Call:
lm(formula = Y ~ X1 + X2, data = cement)

Residuals:
   Min     1Q Median     3Q    Max 
-2.893 -1.574 -1.302  1.363  4.048 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 52.57735    2.28617   23.00 5.46e-10 ***
X1           1.46831    0.12130   12.11 2.69e-07 ***
X2           0.66225    0.04585   14.44 5.03e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.406 on 10 degrees of freedom
Multiple R-squared:  0.9787,    Adjusted R-squared:  0.9744 
F-statistic: 229.5 on 2 and 10 DF,  p-value: 4.407e-09
'

================05a================

# 线性回归模型假设检验
1. residual是否NID.(独立, 等方差, 正态分布)
2. 是否变量之间相关度过高导致多重共线性
3. 异常样本的去除
4. 线性模型是否适合

# 检验正态分布 shapiro.test()
x = rnorm(200)
y = runif(200)
shapiro.test(x)
'
	Shapiro-Wilk normality test

data:  x
W = 0.99543, p-value = 0.8115
'
shapiro.test(y)
'
	Shapiro-Wilk normality test

data:  y
W = 0.95587, p-value = 7.251e-06
'

# 目测是否线性回归适用的例子
Anscombe<-data.frame(
	X=c(10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0),
	Y1=c(8.04,6.95, 7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68),
	Y2=c(9.14,8.14, 8.74,8.77,9.26,8.10,6.13,3.10, 9.13,7.26,4.74),
	Y3=c(7.46,6.77,12.74,7.11,7.81,8.84,6.08,5.39, 8.15,6.44,5.73),
	X4=c(rep(8,7), 19, rep(8,3)),
	Y4=c(6.58,5.76,7.71,8.84,8.47,7.04,5.25,12.50, 5.56,7.91,6.89)
)
plot(Anscombe$X, Anscombe$Y1)

# 取得残差 residuals() / resid()
cement<-data.frame(
	X1=c( 7, 1, 11, 11, 7, 11, 3, 1, 2, 21, 1, 11, 10),
	X2=c(26, 29, 56, 31, 52, 55, 71, 31, 54, 47, 40, 66, 68),
	X3=c( 6, 15, 8, 8, 6, 9, 17, 22, 18, 4, 23, 9, 8),
	X4=c(60, 52, 20, 47, 33, 22, 6, 44, 22, 26, 34, 12, 12),
	Y =c(78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5,93.1,115.9, 83.8, 113.3, 109.4)
)
lm.sol<-lm(Y ~ X1+X2+X3+X4, data=cement)
y.res = residuals(lm.sol)
y.res
'
           1            2            3            4            5            6            7            8            9 
 0.004760418  1.511200700 -1.670937532 -1.727100255  0.250755562  3.925442702 -1.448669087 -3.174988517  1.378349477 
          10           11           12           13 
 0.281547999  1.990983571  0.972989035 -2.294334073 
'
shapiro.test(y.res)
'
        Shapiro-Wilk normality test

data:  y.res
W = 0.9697, p-value = 0.8903
'

# 残差分析例子1
y.rst<-resid(lm.sol)		# 取得残差(使用上例数据)
y.fit<-predict(lm.sol)		# 根据模型及变量值, 计算模拟出来的y值(相对真实的y值)
plot(y.rst~y.fit)			# 观察图, 残差没有明显分布趋势
coef(lm.sol)				# 提取回归系数

# 残差分析例子
z = lm(iris$Sepal.Length ~ iris$Sepal.Width)
plot(resid(z), predict(z))		# 之前手动方法
plot(z)							# 对模型进行快速残差分析

================05b================

# 检测多重共线性的严重程度 kappa()
弱:kappa<100
中:100<kappa<1000
强:1000<kappa

collinear<-data.frame(
	Y=c(10.006, 9.737, 15.087, 8.422, 8.625, 16.289, 5.958, 9.313, 12.960, 5.541, 8.756, 10.937),
	X1=rep(c(8, 0, 2, 0), c(3, 3, 3, 3)),
	X2=rep(c(1, 0, 7, 0), c(3, 3, 3, 3)),
	X3=rep(c(1, 9, 0), c(3, 3, 6)),
	X4=rep(c(1, 0, 1, 10), c(1, 2, 6, 3)),
	X5=c(0.541, 0.130, 2.116, -2.397, -0.046, 0.365, 1.996, 0.228, 1.38, -0.798, 0.257, 0.440),
	X6=c(-0.099, 0.070, 0.115, 0.252, 0.017, 1.504, -0.865, -0.055, 0.502, -0.399, 0.101, 0.432)
)
XX<-cor(collinear[2:7])		# 取出自变量部分, 并求出相关系数矩阵
kappa(XX,exact=TRUE)
' [1] 2195.908 ' # 严重的多重共线性

# 找出多重共线的变量 eigen()
eigen(XX)
'
$values
[1] 2.428787365 1.546152096 0.922077664 0.793984690 0.307892134 0.001106051

$vectors
           [,1]        [,2]        [,3]        [,4]       [,5]         [,6]
[1,] -0.3907189  0.33968212  0.67980398 -0.07990398  0.2510370 -0.447679719
[2,] -0.4556030  0.05392140 -0.70012501 -0.05768633  0.3444655 -0.421140280
[3,]  0.4826405  0.45332584 -0.16077736 -0.19102517 -0.4536372 -0.541689124
[4,]  0.1876590 -0.73546592  0.13587323  0.27645223 -0.0152087 -0.573371872
[5,] -0.4977330  0.09713874 -0.03185053  0.56356440 -0.6512834 -0.006052127
[6,]  0.3519499  0.35476494 -0.04864335  0.74817535  0.4337463 -0.002166594
'
此处最小的eigen value是0.001106051, 对应的eigen vector是t(0.4476, 0.4211, 0.5417, 0.5734, 0.006052, 0.002167)
去除vector较小的后两项, 可以得出X1-X4具有多重共线性

# 广义线性模型 glm()
http://www.ats.ucla.edu/stat/r/dae/logit.htm
norell<-data.frame(x=0:5, n=rep(70,6), success=c(0,9,21,47,60,63))
norell$Ymat<-cbind(norell$success, norell$n - norell$success)

glm.sol<-glm(Ymat~x, family=binomial, data=norell)		# 加上family=binomial即为logistic reg
summary(glm.sol)
'
Call:
glm(formula = Ymat ~ x, family = binomial, data = norell)

Deviance Residuals: 
      1        2        3        4        5        6  
-2.2507   0.3892  -0.1466   1.1080   0.3234  -1.6679  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -3.3010     0.3238  -10.20   <2e-16 ***
x             1.2459     0.1119   11.13   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 250.4866  on 5  degrees of freedom
Residual deviance:   9.3526  on 4  degrees of freedom
AIC: 34.093

Number of Fisher Scoring iterations: 4
'
# P = (exp(−3.3010 + 1.2459X)) / (1 + exp(−3.3010 + 1.2459X))		# X是电流, P是响应为1的概率
# −3.3010 + 1.2459X = ln(P/(1-P))

# 如多变量的logistic reg, 可以用step()做变量删选 R-modeling page 386
life<-data.frame(
	X1=c(2.5, 173, 119, 10, 502, 4, 14.4, 2, 40, 6.6, 21.4, 2.8, 2.5, 6, 3.5, 62.2, 10.8, 21.6, 2, 3.4,
		5.1, 2.4, 1.7, 1.1, 12.8, 1.2, 3.5, 39.7, 62.4, 2.4, 34.7, 28.4, 0.9, 30.6, 5.8, 6.1, 2.7, 4.7, 128, 35,
		2, 8.5, 2, 2, 4.3, 244.8, 4, 5.1, 32, 1.4),
	X2=rep(c(0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0),
			c(1, 4, 2, 2, 1, 1, 8, 1, 5, 1, 5, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 4)),
	X3=rep(c(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1),
			c(6, 1, 3, 1, 3, 1, 1, 5, 1, 3, 7, 1, 1, 3, 1, 1, 2, 9)),
	Y=rep(c(0, 1, 0, 1), c(15, 10, 15, 10))
)
glm.sol<-glm(Y~X1+X2+X3, family=binomial, data=life)
summary(glm.sol)
'
Call:
glm(formula = Y ~ X1 + X2 + X3, family = binomial, data = life)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6960  -0.5842  -0.2828   0.7436   1.9292  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.696538   0.658635  -2.576 0.010000 ** 
X1           0.002326   0.005683   0.409 0.682308    
X2          -0.792177   0.487262  -1.626 0.103998    
X3           2.830373   0.793406   3.567 0.000361 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 67.301  on 49  degrees of freedom
Residual deviance: 46.567  on 46  degrees of freedom
AIC: 54.567

Number of Fisher Scoring iterations: 5
'
# 发现X1的Pr值不显著, 继续用step()进行变量删选(降低model fitness AIC的值)
glm.new<-step(glm.sol)
'
Start:  AIC=54.57
Y ~ X1 + X2 + X3

       Df Deviance    AIC
- X1    1   46.718 52.718
<none>      46.567 54.567
- X2    1   49.502 55.502
- X3    1   63.475 69.475

Step:  AIC=52.72
Y ~ X2 + X3

       Df Deviance    AIC
<none>      46.718 52.718
- X2    1   49.690 53.690
- X3    1   63.504 67.504
'
summary(glm.new)
'
Call:
glm(formula = Y ~ X2 + X3, family = binomial, data = life)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6849  -0.5949  -0.3033   0.7442   1.9073  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -1.6419     0.6381  -2.573 0.010082 *  
X2           -0.7070     0.4282  -1.651 0.098750 .  
X3            2.7844     0.7797   3.571 0.000355 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 67.301  on 49  degrees of freedom
Residual deviance: 46.718  on 47  degrees of freedom
AIC: 52.718

Number of Fisher Scoring iterations: 5
'# P = (exp(−1.6419 − 0.7070X2 + 2.7844X3)) / (1 + exp(−1.6419 − 0.7070X2 + 2.7844X3))

pre<-predict(glm.new, data.frame(X2=2,X3=0))
p<-exp(pre)/(1+exp(pre)); p
'0.04496518'
pre<-predict(glm.new, data.frame(X2=2,X3=1))
p<-exp(pre)/(1+exp(pre)); p
'0.4325522'

================05c================

# 非线性模型
x = c(1.5,2.8,4.5,7.5,10.5,13.5,15.1,16.5,19.5,22.5,24.5,26.5)
y = c(7.0,5.5,4.6,3.6,2.9,2.7,2.5,2.4,2.2,2.1,1.9,1.8)
lm.log = lm(y ~ log(x))		# y = a + b*log(x)
lines(x, fitted(lm.log))

lm.exp = lm(log(y) ~ x)		# y = a*e^(b*x)
lines(x, exp(fitted(lm.exp)))			# 先对lm.exp求拟合值(不对y部分进行拟合), 然后还原为y, 然后以x,y为锚点进行连线

lm.pow = lm(log(y) ~ log(x))		# y = a*x^(b)	幂函数
lines(x, exp(fitted(lm.pow)))
summary(lm.pow)
'
Call:
lm(formula = log(y) ~ log(x))

Residuals:
      Min        1Q    Median        3Q       Max 
-0.054727 -0.020805  0.004548  0.024617  0.045896 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  2.19073    0.02951   74.23 4.81e-15 ***
log(x)      -0.47243    0.01184  -39.90 2.34e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.0361 on 10 degrees of freedom
Multiple R-squared:  0.9938,    Adjusted R-squared:  0.9931 
F-statistic:  1592 on 1 and 10 DF,  p-value: 2.337e-12
'# y = 2.19073*x^(-0.47243)

================06b================

# 关联规则 association rules 挖掘 Aproiri
# 基础概念: 数据挖掘概念与技术.pdf p.135
install.packages("arules")
library(arules)

data(Groceries)		#调用包内置数据集
inspect(Groceries)
' ...
9834 {semi-finished bread,      
      bottled water,            
      soda,                     
      bottled beer}             
9835 {chicken,                  
      tropical fruit,           
      other vegetables,         
      vinegar,                  
      shopping bags} 
'
frequentsets=eclat(Groceries,parameter=list(support=0.05,maxlen=10))	# 查看频繁项集, 大致了解情况
inspect(sort(frequentsets,by="support")[1:10])
'   items                 support
1  {whole milk}       0.25551601
2  {other vegetables} 0.19349263
3  {rolls/buns}       0.18393493
4  {soda}             0.17437722
5  {yogurt}           0.13950178
6  {bottled water}    0.11052364
7  {root vegetables}  0.10899847
8  {tropical fruit}   0.10493137
9  {shopping bags}    0.09852567
10 {sausage}          0.09395018
'	# 有whole milk的transaction占总transaction数的0.25551601

rules = apriori(Groceries, parameter=list(support=0.01, confidence=0.5))	#建立模型, support指定最小支持度(以下为忽略项), confidence指定最小置信度(以下为忽略项)
inspect(rules)
'   lhs                     rhs                   support confidence     lift
1  {curd,                                                                   
    yogurt}             => {whole milk}       0.01006609  0.5823529 2.279125
2  {other vegetables,                                                       
    butter}             => {whole milk}       0.01148958  0.5736041 2.244885
3  {other vegetables,                                                       
    domestic eggs}      => {whole milk}       0.01230300  0.5525114 2.162336
...'# 一个transaction含{curd, yogurt}发生的概率是0.01006609, 发生该transaction时同时会购买whole milk概率为0.5823529
	# lift 是一个类似相关系数的指标, lift=1表示两者相互独立, 数字越大越相关

x=subset(rules, subset=rhs%in%"whole milk"& lift>=1.2)		# 按需求对规则进行删选
inspect(sort(x,by="support")[1:5])
'  lhs                     rhs             support confidence     lift
1 {other vegetables,                                                 
   yogurt}             => {whole milk} 0.02226741  0.5128806 2.007235
2 {tropical fruit,                                                   
   yogurt}             => {whole milk} 0.01514997  0.5173611 2.024770
3 {other vegetables,                                                 
   whipped/sour cream} => {whole milk} 0.01464159  0.5070423 1.984385
4 {root vegetables,                                                  
   yogurt}             => {whole milk} 0.01453991  0.5629921 2.203354
5 {pip fruit,                                                        
   other vegetables}   => {whole milk} 0.01352313  0.5175097 2.025351
'
================07a================

# 线性判别法 lda()
# 用一条直线来划分学习集, 然后根据待测点在直线的哪一边决定它的分类
G=c(rep(1,10), rep(2,10))
x1=c(-1.9,-6.9,5.2,5.0,7.3,6.8,0.9,-12.5,1.5,3.8,0.2,-0.1,0.4,2.7,2.1,-4.6,-1.7,-2.6,2.6,-2.8)
x2=c(3.2,0.4,2.0,2.5,0.0,12.7,-5.4,-2.5,1.3,6.8,6.2,7.5,14.6,8.3,0.8,4.3,10.9,13.1,12.8,10.0)
plot(x1,x2)
text(x1,x2,G,adj=-0.5)

install.packages("MASS")
library("MASS")
ld=lda(G ~ x1+x2)		# 建立模型

z=predict(ld)			# 将x1, x2代入模型进行计算, 如果使用新的测试集, 需要指出新的测试集
newG=z$class
LD1=z$x
y=cbind(G,newG, LD1)	# LD1是判别函数的值, LD1<0,归第1类

================07b================

# 2class的 Gaussian mahalanobis距离判别法
discriminiant.distance <- function(TrnX1, TrnX2, TstX = NULL, var.equal = FALSE){	#Trn表示train; Tst表示test, 默认值为省略; var.equal = FALSE 表示默认协方差不同
	# 无test数据时, test数据由train数据产生
	if (is.null(TstX) == TRUE) TstX <- rbind(TrnX1,TrnX2)
	# test是vector, 转matrix; test是其他类型, 转matrix
	if (is.vector(TstX) == TRUE) TstX <- t(as.matrix(TstX))
	else if (is.matrix(TstX) != TRUE) TstX <- as.matrix(TstX)
	# train转matrix
	if (is.matrix(TrnX1) != TRUE) TrnX1 <- as.matrix(TrnX1)
	if (is.matrix(TrnX2) != TRUE) TrnX2 <- as.matrix(TrnX2)
	
	# 开始比较M距离
	mu1 <- colMeans(TrnX1); mu2 <- colMeans(TrnX2)
	if (var.equal == TRUE || var.equal == T){
		S <- var(rbind(TrnX1, TrnX2))
		w <- mahalanobis(TstX, mu2, S) - mahalanobis(TstX, mu1, S)	# 比较test与两个组之间的M距离
	} else {
		S1 < -var(TrnX1); S2 <- var(TrnX2)
		w <- mahalanobis(TstX, mu2, S2) - mahalanobis(TstX, mu1, S1)
	}
	
	# 构造存储结果的matrix
	nx <- nrow(TstX)
	blong <- matrix(rep(0, nx), nrow=1, byrow=TRUE, dimnames=list("blong", 1:nx))
	# 结果表达转换及写入
	for (i in 1:nx){
		if (w[i] > 0) { blong[i] <- 1 }
		else { blong[i] <- 2 }
	}
	print(blong)
}

# 2class的mahalanobis距离判别法例子 B467
classX1 <- data.frame(
	x1=c(6.60, 6.60, 6.10, 6.10, 8.40, 7.2, 8.40, 7.50, 7.50, 8.30, 7.80, 7.80),
	x2=c(39.00,39.00, 47.00, 47.00, 32.00, 6.0, 113.00, 52.00, 52.00,113.00,172.00,172.00),
	x3=c(1.00, 1.00, 1.00, 1.00, 2.00, 1.0, 3.50, 1.00, 3.50, 0.00, 1.00, 1.50),
	x4=c(6.00, 6.00, 6.00, 6.00, 7.50, 7.0, 6.00, 6.00, 7.50, 7.50, 3.50, 3.00),
	x5=c(6.00, 12.00, 6.00, 12.00, 19.00, 28.0, 18.00, 12.00, 6.00, 35.00, 14.00, 15.00),
	x6=c(0.12, 0.12, 0.08, 0.08, 0.35, 0.3, 0.15, 0.16, 0.16, 0.12, 0.21, 0.21),
	x7=c(20.00,20.00, 12.00, 12.00, 75.00, 30.0, 75.00, 40.00, 40.00,180.00, 45.00, 45.00)
)
classX2 <- data.frame(
	x1=c(8.40, 8.40, 8.40, 6.3, 7.00, 7.00, 7.00, 8.30, 8.30, 7.2, 7.2, 7.2, 5.50, 8.40, 8.40, 7.50, 7.50, 8.30, 8.30, 8.30, 8.30, 7.80, 7.80),
	x2=c(32.0 ,32.00, 32.00, 11.0, 8.00, 8.00, 8.00,161.00, 161.0, 6.0, 6.0, 6.0, 6.00,113.00,113.00, 52.00, 52.00, 97.00, 97.00,89.00,56.00,172.00,283.00),
	x3=c(1.00, 2.00, 2.50, 4.5, 4.50, 6.00, 1.50, 1.50, 0.50, 3.5, 1.0, 1.0, 2.50, 3.50, 3.50, 1.00, 1.00, 0.00, 2.50, 0.00, 1.50, 1.00, 1.00),
	x4=c(5.00, 9.00, 4.00, 7.5, 4.50, 7.50, 6.00, 4.00, 2.50, 4.0, 3.0, 6.0, 3.00, 4.50, 4.50, 6.00, 7.50, 6.00, 6.00, 6.00, 6.00, 3.50, 4.50),
	x5=c(4.00, 10.00, 10.00, 3.0, 9.00, 4.00, 1.00, 4.00, 1.00, 12.0, 3.0, 5.0, 7.00, 6.00, 8.00, 6.00, 8.00, 5.00, 5.00,10.00,13.00, 6.00, 6.00),
	x6=c(0.35, 0.35, 0.35, 0.2, 0.25, 0.25, 0.25, 0.08, 0.08, 0.30, 0.3, 0.3, 0.18, 0.15, 0.15, 0.16, 0.16, 0.15, 0.15, 0.16, 0.25, 0.21, 0.18),
	x7=c(75.00,75.00, 75.00, 15.0,30.00, 30.00, 30.00, 70.00, 70.00, 30.0, 30.0, 30.0,18.00, 75.00, 75.00, 40.00, 40.00,180.00,180.00,180.00,180.00,45.00,45.00)
)

# source("discriminiant.distance.R") # 假设之前距离判别法已存为该script
discriminiant.distance(classX1, classX2, var.equal=TRUE)
'     1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35
blong 1 1 1 1 1 1 1 1 2  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  1  1  2  2  2  2  2  2
'

# 多class的mahalanobis距离判别法
distinguish.distance <- function(TrnX, TrnG, TstX = NULL, var.equal = FALSE){	# 所有train的变量放在TrnX里, 所有train的离散应变量放在TrnG里
	if ( is.factor(TrnG) == FALSE){				# is.factor() 检查是否只含有离散数据
		mx <- nrow(TrnX); mg <- nrow(TrnG)
		TrnX <- rbind(TrnX, TrnG)				# TrnG不是离散数据时, 合并TrnX, TrnG作为新TrnX
		TrnG <- factor(rep(1:2, c(mx, mg)))		# 新TrnG前mx行放1, 后mg行放2 (随便假设的数据, 为了与前程序兼容)
	}
	if (is.null(TstX) == TRUE) TstX <- TrnX
	if (is.vector(TstX) == TRUE) TstX <- t(as.matrix(TstX))
	else if (is.matrix(TstX) != TRUE) TstX <- as.matrix(TstX)
	if (is.matrix(TrnX) != TRUE) TrnX <- as.matrix(TrnX)
	
	nx <- nrow(TstX)
	blong <- matrix(rep(0, nx), nrow=1, dimnames=list("blong", 1:nx))
	g <- length(levels(TrnG))		# levels() 统计有几个水平
	
	mu <- matrix(0, nrow=g, ncol=ncol(TrnX))
	for (i in 1:g) mu[i,] <- colMeans(TrnX[TrnG==i,])
	D < -matrix(0, nrow=g, ncol=nx)
	
	if (var.equal == TRUE || var.equal == T){
		for (i in 1:g)
		D[i,] <- mahalanobis(TstX, mu[i,], var(TrnX))
	} else {
		for (i in 1:g)
		D[i,] <- mahalanobis(TstX, mu[i,], var(TrnX[TrnG==i,]))
	}
	
	for (j in 1:nx){
		dmin <- Inf
		for (i in 1:g){
			if (D[i,j] < dmin){
				dmin <- D[i,j]; blong[j] <- i
			}
		}
	}
	blong
}	# 程序未经过实际检测

# 多class的mahalanobis距离判别法例子 B471
X<-iris[,1:4]
G<-gl(3,50)
source("distinguish.distance.R")
distinguish.distance(X,G)

================07c================

# Gaussian Bayes判别法
# 思路: 最小化 Expected cost of misclassification 误判总损失. (假设为正态分布)
discriminiant.bayes <- function (TrnX1, TrnX2, rate = 1, TstX = NULL, var.equal = FALSE){
	# 无test数据时, test数据由train数据产生
	if (is.null(TstX) == TRUE) TstX<-rbind(TrnX1,TrnX2)
	
	# test是vector, 转matrix; test是其他类型, 转matrix
	if (is.vector(TstX) == TRUE) TstX <- t(as.matrix(TstX))
	else if (is.matrix(TstX) != TRUE) TstX <- as.matrix(TstX)
	
	# train转matrix
	if (is.matrix(TrnX1) != TRUE) TrnX1 <- as.matrix(TrnX1)
	if (is.matrix(TrnX2) != TRUE) TrnX2 <- as.matrix(TrnX2)
	
	# 开始计算最小误判总损失(不再是直接比较M距离), 参考 R-modeling page 473
	mu1 <- colMeans(TrnX1); mu2 <- colMeans(TrnX2)
	if (var.equal == TRUE || var.equal == T){
		S <- var(rbind(TrnX1,TrnX2))
		beta <- 2*log(rate)
		w <- mahalanobis(TstX, mu2, S) - mahalanobis(TstX, mu1, S)
	} else {
		S1 <- var(TrnX1); S2 <- var(TrnX2)
		beta <- 2*log(rate) + log(det(S1)/det(S2))
		w <- mahalanobis(TstX, mu2, S2) - mahalanobis(TstX, mu1, S1)
	}
	
	# 构造存储结果的matrix
	nx <- nrow(TstX)
	blong <- matrix(rep(0, nx), nrow=1, byrow=TRUE, dimnames=list("blong", 1:nx))
	# 结果表达转换及写入
	for (i in 1:nx){
		if (w[i] > beta) blong[i] <- 1
		else blong[i] <- 2
	}
	print(blong)
}

# bayes判别法例子
TrnX1<-matrix(c(24.8, 24.1, 26.6, 23.5, 25.5, 27.4, -2.0, -2.4, -3.0, -1.9, -2.1, -3.1), ncol=2)
TrnX2<-matrix(c(22.1, 21.6, 22.0, 22.8, 22.7, 21.5, 22.1, 21.4,
				-0.7, -1.4, -0.8, -1.6, -1.5, -1.0, -1.2, -1.3), ncol=2)
source("discriminiant.bayes.R")
discriminiant.bayes(TrnX1, TrnX2, rate=8/6, var.equal=TRUE)
# rate = (L(1/2)*p2)/(L(2/1)*p1) 由业务知识得这个值
'     1 2 3 4 5 6 7 8 9 10 11 12 13 14
blong 1 1 1 2 1 1 2 2 2  2  2  2  2  2
'

================08a================

# KNN 算法 (非连续变量可用)
install.packages("FNN")
library("FNN")
mydata <-read.csv("E:/graduate project/wk2 logit/data.csv")

train <- rbind(mydata[,1:4])
cl <-rbind(mydata[,5])
test <- cbind (2, 3, 4, 5)

test.knn <- knn(train, test, cl, k = 5, prob=TRUE)

# 决策树 decision tree 原理  (仅非连续变量可用)
step1, 信息增益information gain的计算 18:30-24:06

step2,
日志密度的信息增益是0.276
真实头像的信息增益为0.033
好友密度的信息增益为0.553
因为好友密度F具有最大的信息增益，所以第一次分裂选择F为分裂属性

# 决策树
install.packages("rpart")
library("rpart")

iris.rp = rpart(Species~., data=iris, method="class")		# Species 对 其他所有列 建模
plot(iris.rp, uniform=T, branch=0, margin=0.1, main="Classification Tree\nIris Species by Petal and Sepal Length")
text(iris.rp, use.n=T, fancy=T, col="blue")

iris.rp
'
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  
  2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
  3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  
    6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
    7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
'
================08b================

# 神经网络 Artificial Neural Networks (ANN)

install.packages("AMORE")
library(AMORE)

x1=c(1,1,1,1,0,0,0,0)
x2=c(0,0,1,1,0,1,1,0)
x3=c(0,1,0,1,1,0,1,0)
y=c(-1,1,1,1,-1,-1,1,-1)

P=cbind(x1,x2,x3)
target=y

net <- newff(n.neurons=c(3,1,1),			# 指定各层神经元个数,c(输入层, 中间层, 输出层)
			learning.rate.global=1e-2,		# 学习速度
			momentum.global=0.4,			# Momentum for every neuron. Needed by several training methods.
			error.criterium="LMS",			# 判断收敛依据方法, "LMS": Least Mean Squares.
			hidden.layer="tansig",			# 用于激活隐藏层的函数, tansig 见slide272
			output.layer="purelin",			# 用于激活输出层的函数, purelin 见slide270-271
			method="ADAPTgdwm"				# Prefered training method, ADAPTgdwm": Adaptative gradient descend with momentum.
		)

result <- train(net, P, target, error.criterium="LMS", report=TRUE, show.step=100, n.shows=5)
'
index.show: 1 LMS 0.204195868509906 
index.show: 2 LMS 0.19042333982356 
index.show: 3 LMS 0.175447652319941 
index.show: 4 LMS 0.159176104814864 
index.show: 5 LMS 0.142610075897662 
'
z <- sim(result$net, P)	# Performs the simulation of a neural network from an input data set
cbind(z,y)
'                y
[1,] -0.6316873 -1
[2,]  0.6864095  1
[3,]  0.6872219  1
[4,]  1.3698263  1
[5,] -0.6277175 -1
[6,] -0.6267398 -1
[7,]  0.6905184  1
[8,] -1.5464335 -1
'

================09a================

# 支持向量机 support vector machine (SVM)
# 算法概念: 12:05-14:32 Objective is to Maximize the margins

# SVM 例子
http://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf

install.packages("e1071")		# SVM在这个包中
library("e1071")

install.packages("mlbench")		# 只是使用其中一个数据表
data(Glass, package="mlbench")

index <- 1:nrow(Glass)			# split data into a train and test set
testindex <- sample(index, trunc(length(index)/3))
testset <- Glass[testindex,]
trainset <- Glass[-testindex,]

svm.model <- svm(Type ~ ., data = trainset, cost = 100, gamma = 1)		# svm
svm.pred <- predict(svm.model, testset[,-10])

table(pred = svm.pred, true = testset[,10])
'   true
pred  1  2  3  5  6  7
   1 19  1  2  0  0  0
   2  6 17  3  8  1  5
   3  0  1  0  0  0  0
   5  0  0  0  1  0  0
   6  0  0  0  0  1  0
   7  0  0  0  0  0  6
'
================09b================

# 求各种距离 dist()
x1=c(1,2,3,4,5)
x2=c(3,2,1,4,6)
x3=c(5,3,5,6,2)
x=data.frame(x1,x2,x3)

dist(x, method="euclidean")					# 总共5个3维点
'        1        2        3        4
2 2.449490                           
3 2.828427 2.449490                  
4 3.316625 4.123106 3.316625         
5 5.830952 5.099020 6.164414 4.582576
'
dist(x, method="minkowski", p=5)
dist(x, method="binary")

# 数据中心化, 标准化 scale()
# 极差化 sweep() 标准化除以stdev, 这里除以range

# hierarchical cluster 聚类 hclust()
x<-c(1,2,6,8,11); dim(x)<-c(5,1);						# 生成一个5x1矩阵
d<-dist(x)												# 生成距离对象
'd
   1  2  3  4
2  1         
3  5  4      
4  7  6  2   
5 10  9  5  3
'
hc1<-hclust(d, "single"); hc2<-hclust(d, "complete")	# 最短, 最长法. 传入的对象不是点, 要用点之间距离
hc3<-hclust(d, "centroid"); hc4<-hclust(d, "average")

plot(hc1, hang=-1)
rect.hclust(hc1, k=3)									# R中没有判定分类个数的计算方法, 需指定分类数目(SAS中有RMSSTD等指标可用)

opar <- par(mfrow = c(2, 2))							# 出图的布局设置
	plot(hc1,hang=-1); plot(hc2,hang=-1)				# 产生dendrogram图	(谱系图)
	plot(hc3,hang=-1); plot(hc4,hang=-1)
par(opar)

dend1<-as.dendrogram(hc1)								# 绘图控制示例
opar <- par(mfrow = c(2, 2), mar = c(4,3,1,2))			# mar 设置每个图形的bottom, left, top, right的padding
	plot(dend1)
	plot(dend1, nodePar=list(pch = c(1,NA), cex=0.8, lab.cex=0.8), type = "t", center=TRUE)	# 
	plot(dend1, edgePar=list(col = 1:2, lty = 2:3), dLeaf=1, edge.root = TRUE)
	plot(dend1, nodePar=list(pch = 2:1, cex=.4*2:1, col=2:3), horiz=TRUE)
par(opar)

# 聚类 hclust() 例子 B507
X<-data.frame(
	x1=c(2959.19, 2459.77, 1495.63, 1046.33, 1303.97, 1730.84, 1561.86, 1410.11, 3712.31, 2207.58, 2629.16, 1844.78, 2709.46, 1563.78, 1675.75, 1427.65, 1783.43, 1942.23, 3055.17, 2033.87, 2057.86, 2303.29, 1974.28, 1673.82, 2194.25, 2646.61, 1472.95, 1525.57, 1654.69, 1375.46, 1608.82),
	x2=c(730.79, 495.47, 515.90, 477.77, 524.29, 553.90, 492.42, 510.71, 550.74, 449.37, 557.32, 430.29, 428.11, 303.65, 613.32, 431.79, 511.88, 512.27, 353.23, 300.82, 186.44, 589.99, 507.76, 437.75, 537.01, 839.70, 390.89, 472.98, 437.77, 480.99, 536.05),
	x3=c(749.41, 697.33, 362.37, 290.15, 254.83, 246.91, 200.49, 211.88, 893.37, 572.40, 689.73, 271.28, 334.12, 233.81, 550.71, 288.55, 282.84, 401.39, 564.56, 338.65, 202.72, 516.21, 344.79, 461.61, 369.07, 204.44, 447.95, 328.90, 258.78, 273.84, 432.46),
	x4=c(513.34, 302.87, 285.32, 208.57, 192.17, 279.81, 218.36, 277.11, 346.93, 211.92, 435.69, 126.33, 160.77, 107.90, 219.79, 208.14, 201.01, 206.06, 356.27, 157.78, 171.79, 236.55, 203.21, 153.32, 249.54, 209.11, 259.51, 219.86, 303.00, 317.32, 235.82),
	x5=c(467.87, 284.19, 272.95, 201.50, 249.81, 239.18, 220.69, 224.65, 527.00, 302.09, 514.66, 250.56, 405.14, 209.70, 272.59, 217.00, 237.60, 321.29, 811.88, 329.06, 329.65, 403.92, 240.24, 254.66, 290.84, 379.30, 230.61, 206.65, 244.93, 251.08, 250.28),
	x6=c(1141.82, 735.97, 540.58, 414.72, 463.09, 445.20, 459.62, 376.82, 1034.98, 585.23, 795.87, 513.18, 461.67, 393.99, 599.43, 337.76, 617.74, 697.22, 873.06, 621.74, 477.17, 730.05, 575.10, 445.59, 561.91, 371.04, 490.90, 449.69, 479.53, 424.75, 541.30),
	x7=c(478.42, 570.84, 364.91, 281.84, 287.87, 330.24, 360.48,317.61, 720.33, 429.77, 575.76, 314.00, 535.13, 509.39, 371.62, 421.31, 523.52, 492.60, 1082.82, 587.02, 312.93, 438.41, 430.36, 346.11, 407.70, 269.59, 469.10, 249.66, 288.56, 228.73, 344.85),
	x8=c(457.64, 305.08, 188.63, 212.10, 192.96, 163.86, 147.76, 152.85, 462.03, 252.54, 323.36, 151.39, 232.29, 160.12, 211.84, 165.32, 182.52, 226.45, 420.81, 218.27, 279.19, 225.80, 223.46, 191.48, 330.95, 389.33, 191.34, 228.19, 236.51, 195.93, 214.40)
)
d <- dist(scale(X))
hc1 <- hclust(d); hc2 <- hclust(d, "ward.D")
 
opar<-par(mfrow=c(2,1), mar=c(5.2,4,0,0))
	plot(hc1, hang=-1); rect.hclust(hc1, k=5, border="red")
	plot(hc2, hang=-1); rect.hclust(hc2, k=5, border="blue")
par(opar)

================10a================

# K-means 聚类 kmeans()				# 不断调整新的中心点
x = iris[,1:4]
km = kmeans(x, 3)		# 传入的对象不是点之间距离, 要直接用点(与hclust()不同). 3代表分类数
km
'K-means clustering with 3 clusters of sizes 38, 50, 62

Cluster means:
  Sepal.Length Sepal.Width Petal.Length Petal.Width
1     6.850000    3.073684     5.742105    2.071053
2     5.006000    3.428000     1.462000    0.246000
3     5.901613    2.748387     4.393548    1.433871

Clustering vector:
  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [31] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 1 3 3 3 3 3 3 3
 [61] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3
 [91] 3 3 3 3 3 3 3 3 3 3 1 3 1 1 1 1 3 1 1 1 1 1 1 3 3 1 1 1 1 3
[121] 1 3 1 3 1 1 3 3 1 1 1 1 1 3 1 1 1 1 3 1 1 1 3 1 1 1 3 1 1 3

Within cluster sum of squares by cluster:
[1] 23.87947 15.15100 39.82097
 (between_SS / total_SS =  88.4 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"    
[5] "tot.withinss" "betweenss"    "size"         "iter"        
[9] "ifault"  
'

# K-center 聚类 pam()		# 计算变更中心点的代价
install.packages("cluster")
library(cluster)

x = iris[,1:4]
kc = pam(x, 3)
kc
'Medoids:
      ID Sepal.Length Sepal.Width Petal.Length Petal.Width
[1,]   8          5.0         3.4          1.5         0.2
[2,]  79          6.0         2.9          4.5         1.5
[3,] 113          6.8         3.0          5.5         2.1
Clustering vector:
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [39] 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [77] 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 3 3 3 3 2 3 3 3 3 3 3 2
[115] 2 3 3 3 3 2 3 2 3 2 3 3 2 2 3 3 3 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 2 3 3 2
Objective function:
    build      swap 
0.6709391 0.6542077 

Available components:
 [1] "medoids"    "id.med"     "clustering" "objective"  "isolation" 
 [6] "clusinfo"   "silinfo"    "diss"       "call"       "data" 
'

================10b================

# Density-Based Spatial Clustering of Applications with Noise (DBSCAN) 赞!
http://cran.r-project.org/web/packages/fpc/fpc.pdf

install.packages("fpc")
library(fpc)

set.seed(665544)
n <- 600
x <- cbind(runif(10, 0, 10)+rnorm(n, sd=0.2), runif(10, 0, 10)+rnorm(n, sd=0.2))		# generate 10 groups of data, total 600 observations.

ds <- dbscan(x, 0.2)			# 0.2 is reachability distance, use default MinPts = 5
ds
'
dbscan Pts=600 MinPts=5 eps=0.2
        0  1  2  3  4  5  6  7  8  9 10 11
border 28  4  4  8  5  3  3  4  3  4  6  4
seed    0 50 53 51 52 51 54 54 54 53 51  1
total  28 54 57 59 57 54 57 58 57 57 57  5
'
par(bg="grey40")
plot(ds, x)

================11c================

# principal component analysis (PCA) 主成分分析		B518
# 某PC(i)解释了总信息比例(贡献率): eigenvalue(i)/sum(eigenvalue)
# 原变量x(j)与PC的转换计算: x = eigenvector * PC(i) 	#???未确认???
# 某PC解释了x(i)的比例: x与PC复相关系数的平方
# 某原变量x(i)对PC的影响(loading)

================11d================

# 主成分分析 princomp()		B525, 用于去除重复信息, dimension reduction, 无需应变量
student <- data.frame(
	X1=c(148, 139, 160, 149, 159, 142, 153, 150, 151, 139, 140, 161, 158, 140, 137, 152, 149, 145, 160, 156, 151, 147, 157, 147, 157, 151, 144, 141, 139, 148),
	X2=c(41, 34, 49, 36, 45, 31, 43, 43, 42, 31, 29, 47, 49, 33, 31, 35, 47, 35, 47, 44, 42, 38, 39, 30, 48, 36, 36, 30, 32, 38),
	X3=c(72, 71, 77, 67, 80, 66, 76, 77, 77, 68, 64, 78, 78, 67, 66, 73, 82, 70, 74, 78, 73, 73, 68, 65, 80, 74, 68, 67, 68, 70),
	X4=c(78, 76, 86, 79, 86, 76, 83, 79, 80, 74, 74, 84, 83, 77, 73, 79, 79, 77, 87, 85, 82, 78, 80, 75, 88, 80, 76, 76, 73, 78)
)

student.pr <- princomp(student, cor = TRUE)		# 所有自变量全部参加PC分析
summary(student.pr, loadings=TRUE)
'
Importance of components:
                          Comp.1     Comp.2     Comp.3     Comp.4
Standard deviation     1.8817805 0.55980636 0.28179594 0.25711844
Proportion of Variance 0.8852745 0.07834579 0.01985224 0.01652747	# 单项贡献
Cumulative Proportion  0.8852745 0.96362029 0.98347253 1.00000000	# 累计贡献

Loadings:
   Comp.1 Comp.2 Comp.3 Comp.4
X1 -0.497  0.543 -0.450  0.506
X2 -0.515 -0.210 -0.462 -0.691
X3 -0.481 -0.725  0.175  0.461
X4 -0.507  0.368  0.744 -0.232
'	# PC1 = -0.497*X1 + -0.515*X2 + -0.481*X3 + -0.507*X4, 然后interpret其代表意义

predict(student.pr)
'          Comp.1      Comp.2      Comp.3       Comp.4
 [1,]  0.06990950 -0.23813701 -0.35509248 -0.266120139
 [2,]  1.59526340 -0.71847399  0.32813232 -0.118056646
...
[29,]  2.40434827 -0.48613137 -0.16154441 -0.007914021
[30,]  0.50287468  0.14734317 -0.20590831 -0.122078819
'	# 原来的30个obs用PC表示式(其实只需要PC1, 其解释了88%的variances)

screeplot(student.pr,type="lines")

# princomp()例子
PCA = princomp(X, cor=T)
'Call:	# 使用之前cluster中例子数据
princomp(x = X, cor = T)

Standard deviations:
   Comp.1    Comp.2    Comp.3    Comp.4    Comp.5    Comp.6    Comp.7    Comp.8 
2.2556395 1.1632889 0.7567221 0.6376603 0.5278638 0.3502837 0.3063912 0.2905094 

 8  variables and  31 observations.
'
summary(PCA, loadings=T)
'Importance of components:
                          Comp.1    Comp.2     Comp.3     Comp.4     Comp.5     Comp.6     Comp.7     Comp.8
Standard deviation     2.2556395 1.1632889 0.75672212 0.63766032 0.52786385 0.35028368 0.30639121 0.29050938
Proportion of Variance 0.6359887 0.1691551 0.07157855 0.05082633 0.03483003 0.01533733 0.01173445 0.01054946
Cumulative Proportion  0.6359887 0.8051438 0.87672239 0.92754873 0.96237876 0.97771609 0.98945054 1.00000000

Loadings:
   Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8
x1  0.399         0.416  0.214 -0.217        -0.280  0.693
x2  0.132  0.749  0.339  0.157  0.523                     
x3  0.375        -0.444  0.544        -0.562 -0.161 -0.121
x4  0.320  0.346 -0.475 -0.657                       0.335
x5  0.388 -0.231  0.282 -0.364  0.210 -0.109 -0.566 -0.456
x6  0.406        -0.308  0.234         0.795        -0.229
x7  0.327 -0.495                0.582         0.514  0.182
x8  0.396         0.338 -0.116 -0.538 -0.127  0.551 -0.312
'		# 可以看出只需要PC1, PC2就可以代表80%的viriances, 从而进行cluster分析

kmeans(predict(PCA)[,1:2], 5)

# 主成分回归例子	B533
conomy<-data.frame(
	x1=c(149.3, 161.2, 171.5, 175.5, 180.8, 190.7, 202.1, 212.4, 226.1, 231.9, 239.0),
	x2=c(4.2, 4.1, 3.1, 3.1, 1.1, 2.2, 2.1, 5.6, 5.0, 5.1, 0.7),
	x3=c(108.1, 114.8, 123.2, 126.9, 132.1, 137.7, 146.0, 154.1, 162.3, 164.3, 167.6),
	y=c(15.9, 16.4, 19.0, 19.1, 18.8, 20.4, 22.7, 26.5, 28.1, 27.6, 26.3)
)

# step1, 直接用线性回归
lm.sol<-lm(y~x1+x2+x3, data=conomy)		
summary(lm.sol)
'Call:
lm(formula = y ~ x1 + x2 + x3, data = conomy)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.52367 -0.38953  0.05424  0.22644  0.78313 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -10.12799    1.21216  -8.355  6.9e-05 ***
x1           -0.05140    0.07028  -0.731 0.488344    
x2            0.58695    0.09462   6.203 0.000444 ***
x3            0.28685    0.10221   2.807 0.026277 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4889 on 7 degrees of freedom
Multiple R-squared:  0.9919,	Adjusted R-squared:  0.9884 
F-statistic: 285.6 on 3 and 7 DF,  p-value: 1.112e-07
'	# 变量中存在多重共线性, 导致模型失真

# step2, 进行PCA
conomy.pr<-princomp(~x1+x2+x3, data=conomy, cor=T)
summary(conomy.pr, loadings=TRUE)
'Importance of components:
                         Comp.1    Comp.2       Comp.3
Standard deviation     1.413915 0.9990767 0.0518737839
Proportion of Variance 0.666385 0.3327181 0.0008969632
Cumulative Proportion  0.666385 0.9991030 1.0000000000		# 前两个PC已经解释了99.9%的variances

Loadings:
   Comp.1 Comp.2 Comp.3
x1 -0.706         0.707
x2        -0.999       
x3 -0.707        -0.707
'

# step3, 使用PC1, PC2进行线性回归
pre<-predict(conomy.pr)
conomy$z1<-pre[,1]; conomy$z2<-pre[,2]

lm.sol<-lm(y~z1+z2, data=conomy)
summary(lm.sol)
'Call:
lm(formula = y ~ z1 + z2, data = conomy)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.89838 -0.26050  0.08435  0.35677  0.66863 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  21.8909     0.1658 132.006 1.21e-14 ***
z1           -2.9892     0.1173 -25.486 6.02e-09 ***
z2           -0.8288     0.1660  -4.993  0.00106 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.55 on 8 degrees of freedom
Multiple R-squared:  0.9883,	Adjusted R-squared:  0.9853 
F-statistic: 337.2 on 2 and 8 DF,  p-value: 1.888e-08
'
================12b================

# factor analysis	从相关性较强的组合中找出难以测量的potential factors, 然后用公共因子与个性因子的组合来描述原变量

# cov(xi, fi) 即aij, 代表xi可以由f1, f2...fm线性组合的方式		B541
# hi^2 = sum(aij)^2, 代表公共因子对原变量xi的影响(common variance)
# hi^2 + s^2 =1, 这里s^2 代表个性对原变量的影响(specifie variance)

================12c================

# 因子分析 factanal()	11:40
x <- c(1.000,
		0.923, 1.000,
		0.841, 0.851, 1.000,
		0.756, 0.807, 0.870, 1.000,
		0.700, 0.775, 0.835, 0.918, 1.000,
		0.619, 0.695, 0.779, 0.864, 0.928, 1.000,
		0.633, 0.697, 0.787, 0.869, 0.935, 0.975, 1.000,
		0.520, 0.596, 0.705, 0.806, 0.866, 0.932, 0.943, 1.000		# 这是一个cov matrix
	   )
names <- c("X1", "X2", "X3", "X4", "X5", "X6", "X7", "X8")
R <- matrix(0, nrow=8, ncol=8, dimnames=list(names, names))
for (i in 1:8){
	for (j in 1:i){
		R[i,j]<-x[(i-1)*i/2+j]; R[j,i]<-R[i,j]
	}
}

fa<-factanal(factors=2, covmat=R); fa
'Call:
factanal(factors = 2, covmat = R)

Uniquenesses:
   X1    X2    X3    X4    X5    X6    X7    X8 
0.081 0.075 0.152 0.135 0.082 0.033 0.018 0.087 

Loadings:
   Factor1 Factor2
X1 0.291   0.913  
X2 0.382   0.883  
X3 0.543   0.744  
X4 0.691   0.622  
X5 0.799   0.529  
X6 0.901   0.393  
X7 0.907   0.399  
X8 0.914   0.278  

               Factor1 Factor2
SS loadings      4.112   3.225
Proportion Var   0.514   0.403
Cumulative Var   0.514   0.917

The degrees of freedom for the model is 13 and the fit was 0.3318 
' # Factor1主要和x6-8有关, 可以根据业务知道代表耐力; Factor2主要和X1-3有关, 和爆发力有关